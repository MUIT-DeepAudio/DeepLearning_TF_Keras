{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TSA_IntroTF_2.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"cFntciKlhH1C","colab_type":"text"},"cell_type":"markdown","source":["#  From <font color='organge'>\"manual\"</font> learning to <font color='brown'>\"machine\"</font> learning\n","## A Simple linear classifier in [TensorFlow](https://www.tensorflow.org/) (logistic regression)\n","\n","<img src=\" https://www.skylinelabs.in/blog/images/tensorflow.jpg\">\n","\n","\n","<font size=5 color='green'>MUIT-TSA Acoustic Signal Processing:</font><br>\n","\n"]},{"metadata":{"id":"a79ndaP9hH1G","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import tensorflow as tf"],"execution_count":0,"outputs":[]},{"metadata":{"id":"sS2gcP7GhH1R","colab_type":"text"},"cell_type":"markdown","source":["### Generate two-class artificial data using $numpy$"]},{"metadata":{"id":"SbQcje4XhH1T","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# Number of data per-class\n","Ndata_class=100\n","\n","group1 = np.random.multivariate_normal([-4, -4], 20*np.identity(2), size=Ndata_class)\n","group2 = np.random.multivariate_normal([4, 4], 20*np.identity(2), size=Ndata_class)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EcGshEKJhH1a","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Plot artificial data\n","plt.figure(figsize=(9,6))\n","plt.scatter(group1.T[0][:],group1.T[1][:])\n","plt.scatter(group2.T[0][:],group2.T[1][:],color='g')\n","plt.xlabel('x1',fontsize=18)\n","plt.ylabel('x2',fontsize=18)\n","plt.title('Artificial Data',fontsize=18)\n","plt.grid(color='k', linestyle='--')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Hpk6OlXUhH1m","colab_type":"text"},"cell_type":"markdown","source":["### \"manual\" linear discrimination\n","* x2= w1 * x1 + b\n","<br>or\n","* x2 - w1*x1 - b = 0"]},{"metadata":{"id":"R4Ei_4MZhH1o","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["x1 = np.arange(-15, 15, 0.1)\n","\n","b=0.0\n","w1=-1.0\n","#w1=1.0\n","\n","x2= w1 * x1 + b "],"execution_count":0,"outputs":[]},{"metadata":{"id":"_xLan16nhH1t","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Plot linear discrimination\n","plt.figure(figsize=(9,6))\n","plt.scatter(group1.T[0][:],group1.T[1][:])\n","plt.scatter(group2.T[0][:],group2.T[1][:],color='g')\n","plt.plot(x1,x2,color='r')\n","plt.xlabel('x1',fontsize=18)\n","plt.ylabel('x2',fontsize=18)\n","plt.title('Linear Discrimination',fontsize=18)\n","plt.grid(color='k', linestyle='--')\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MkXt37LNhH12","colab_type":"text"},"cell_type":"markdown","source":["### Classification based on the distance to the line\n","\n","* x2 - w1*x1 - b > 0  'green' points\n","* x2 - w1*x1 - b < 0  'blue' points\n"]},{"metadata":{"id":"xmhS_LkPhH14","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train_X = np.vstack((group1, group2))\n","pred= train_X.T[1] - w1 * train_X.T[0] - b \n","\n","plt.figure(figsize=(9,6))\n","plt.plot(pred)\n","plt.xlabel('x1',fontsize=18)\n","plt.ylabel('x2',fontsize=18)\n","plt.title('Classification: < 0 \\'blue\\' > 0 \\'green\\' ',fontsize=18)\n","plt.grid(color='k', linestyle='--')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Jvyg0F60hH2D","colab_type":"text"},"cell_type":"markdown","source":["### let's make these values \"probabilities\" using the $sigmoid$ function"]},{"metadata":{"id":"VBU58W5ihH2F","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["x = np.arange(-10, 11)\n","plt.title('Sigmoid : logistic function',fontsize=18)\n","plt.xlabel('$x$')\n","plt.ylabel('$p(X)=1/(1+exp(-x))$',fontsize=16)\n","plt.plot(x, (1/(1+np.exp(-x))));\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"O2oyzikyhH2V","colab_type":"text"},"cell_type":"markdown","source":["####     Prediction using the $logistic$ or $sigmoid$ function \n","* $p(X) = 1/(1 + \\exp(x))$, taking values between $0$ and $1$.\n","\n","* $p(X)$ represents the probability that the point $X$ should be labelled \"green\".\n"]},{"metadata":{"id":"wrCgCeWBhH2X","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["plt.figure(figsize=(9,6))\n","plt.plot((1/(1+np.exp(-pred))))\n","plt.ylim(-0.02, 1.02)\n","\n","plt.title('Class-probabilities',fontsize=18)\n","plt.xlabel('$pred$',fontsize=16)\n","plt.ylabel('$p(X)=1/(1+exp(-pred))$',fontsize=16)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vqi9-4lnhH2k","colab_type":"text"},"cell_type":"markdown","source":["###  Classification cost function will be the cross-entropy, $$-\\sum_{i=1}^n l(X_i) \\log(p(X_i)) + (1-l(X_i))\\log(1-p(X_i)),$$ where $l(X_i)$ is the label of $X_i$ (which is $0$ for 'blue' or $1$ for 'green').\n"]},{"metadata":{"id":"xBjPtk2fhH2l","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["train_labels = np.array([0.0] * Ndata_class + [1.0] * Ndata_class)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"thkvoCpfhH2s","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Cost function is cross-entropy\n","pred_prob=(1/(1+np.exp(-pred)))\n","cost = -sum((train_labels) * np.log(pred_prob + 1e-10) + (1-train_labels) * np.log(1-pred_prob + 1e-10))\n","\n","print(\"cross-entropy: {}\".format(cost))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ovF9CTaIhH21","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["Accuracy=np.sum((pred_prob>0.5).astype(int) == train_labels)/(len(pred_prob)*1.0)\n","\n","print(\"Classification Accuracy = \",Accuracy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"l9iUnBLChH28","colab_type":"text"},"cell_type":"markdown","source":["## Using TensorFlow for:\n","\n","- ### Feeding data into a $given$ linear classifier with sigmoid output"]},{"metadata":{"id":"sXg5nqW9hH2_","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### GRAPH DEFINITION\n","\n","# PLACEHOLDERS:\n","# Inputs are now two-dimensional and come with labels \"blue\" or \"green\" (represented by 0 or 1)\n","X = tf.placeholder(\"float\", shape=[None, 2])\n","labels = tf.placeholder(\"float\", shape=[None])\n","\n","\n","# Set model weights and bias as before\n","#W = tf.Variable(tf.ones([2, 1], \"float\"), name=\"weight\")\n","#b = tf.Variable(tf.zeros([1], \"float\"), name=\"bias\")\n","\n","W=tf.constant([[1.0], [1.0]],name=\"weights\")\n","b=tf.constant(0.0,name=\"bias\")\n","\n","\n","# Predictor is now the logistic function\n","#pred = tf.sigmoid(tf.to_double(tf.reduce_sum(tf.matmul(X, W), axis=[1]) + b))\n","pred = tf.sigmoid(tf.to_double(tf.reduce_sum(tf.matmul(X, W),1) + b))\n","\n","# Cost function is cross-entropy\n","cost = -tf.reduce_sum(tf.to_double(labels) * tf.log(pred) + (1-tf.to_double(labels)) * tf.log(1-pred))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ukjgwui3hH3G","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["### GRAPH EXECUTION\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","#init = tf.initialize_all_variables()\n","\n","# We stack our two groups of 2-dimensional points\n","train_X = np.vstack((group1, group2))\n","\n","# labels to feed them\n","train_labels = np.array([0.0] * Ndata_class + [1.0] * Ndata_class)\n","\n","\n","with tf.Session() as sess:\n","    \n","    sess.run(init)\n","    \n","    pred_out, cost_out=sess.run([pred, cost], feed_dict={X: train_X, labels: train_labels})\n","\n","        "],"execution_count":0,"outputs":[]},{"metadata":{"id":"9N55Adv8hH3P","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["print(\"cross-entropy: {}\".format(cost_out))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"I3ysOnxThH3m","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["Accuracy=np.sum((pred_out>0.5).astype(int) == train_labels)/(len(pred_out)*1.0)\n","\n","print(\"Classification Accuracy = \",Accuracy)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cv-m0wbDhH3s","colab_type":"text"},"cell_type":"markdown","source":["# Now let's train!\n","\n","\n","\n","*   Both $W$ and $b$ are now <font size=4 color=green> variables</font>\n","\n","\n"]},{"metadata":{"id":"m__uHgZ5hH3t","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# Inputs are now two-dimensional and come with labels \"blue\" or \"green\" (represented by 0 or 1)\n","X = tf.placeholder(\"float\", shape=[None, 2])\n","labels = tf.placeholder(\"float\", shape=[None])\n","\n","# Set model weights and bias as before\n","W = tf.Variable(tf.zeros([2, 1], \"float\"), name=\"weight\")\n","b = tf.Variable(tf.zeros([1], \"float\"), name=\"bias\")\n","\n","# Predictor is now the logistic function\n","#pred = tf.sigmoid(tf.to_double(tf.reduce_sum(tf.matmul(X, W), axis=[1]) + b))\n","pred = tf.sigmoid(tf.to_double(tf.reduce_sum(tf.matmul(X, W),1) + b))\n","\n","\n","# Cost function is cross-entropy\n","cost = -tf.reduce_sum(tf.to_double(labels) * tf.log(pred) + (1-tf.to_double(labels)) * tf.log(1-pred))\n","\n","# Gradient descent\n","learning_rate = 0.001\n","optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n","\n","# Initializing the variables\n","init = tf.global_variables_initializer()\n","#init = tf.initialize_all_variables()\n","\n","# We stack our two groups of 2-dimensional points\n","train_X = np.vstack((group1, group2))\n","\n","# labels to feed them\n","train_labels = np.array([0.0] * Ndata_class + [1.0] * Ndata_class)\n","\n","\n","with tf.Session() as sess:\n","    sess.run(init)\n","    \n","    # We can Run the optimization algorithm several times\n","    for i in range(10):\n","        cost_out,W_out,b_out,pred_out,_=sess.run([cost, W,b, pred, optimizer], feed_dict={X: train_X, labels: train_labels})\n","        print(\"\\n***** Epoch : %d \\n Cost= %s \"%(i,cost_out))\n","        print(\"Weights= \",format(W_out))\n","        print(\"bias= \",format(b_out))\n","        \n","        Accuracy=np.sum((pred_out>0.5).astype(int) == train_labels)/(len(pred_out)*1.0)\n","\n","        print(\"Classification Accuracy = \",Accuracy)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WQfPDBCRhH4G","colab_type":"text"},"cell_type":"markdown","source":["### Now we train using batches\n"]},{"metadata":{"id":"Bpe9JNc-hH4K","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["import matplotlib.cm as cm\n","import seaborn as sns\n","\n","n_samples=Ndata_class*2\n","batch_size=40\n","\n","with tf.Session() as sess:\n","    # We stack our two groups of 2-dimensional points and label them 0 and 1 respectively\n","    train_X = np.vstack((group1, group2))\n","\n","    # labels to feed them\n","    train_labels = np.array([0.0] * Ndata_class + [1.0] * Ndata_class)\n","\n","\n","    sess.run(init)\n","\n","    # Run the optimization algorithm 1000 times\n","    for i in range(1000):\n","        # Select random minibatch\n","        indices = np.random.choice(n_samples, batch_size)\n","        X_batch, labels_batch = train_X[indices], train_labels[indices]\n","        sess.run(optimizer, feed_dict={X: X_batch, labels: labels_batch})\n","\n","        \n","    # Plot the predictions: the values of p\n","    Xmin = np.min(train_X)-1\n","    Xmax = np.max(train_X)+1\n","    x = np.arange(Xmin, Xmax, 0.1)\n","    y = np.arange(Xmin, Xmax, 0.1)\n","    \n","\n","    plt.scatter(group1.T[0][:],group1.T[1][:])\n","    plt.scatter(group2.T[0][:],group2.T[1][:],color='g')\n","    plt.xlim(Xmin, Xmax)\n","    plt.ylim(Xmin, Xmax)\n","    print('W = ', sess.run(W))\n","    print('b = ', sess.run(b))\n","    \n","    xx, yy = np.meshgrid(x, y)\n","    predictions = sess.run(pred, feed_dict={X: np.array((xx.ravel(), yy.ravel())).T})\n","    \n","    plt.title('Probability that model will label a given point \"green\"')\n","    plt.contour(x, y, predictions.reshape(len(x), len(y)), cmap=cm.BuGn, levels=np.arange(0.0, 1.1, 0.1))\n","    plt.colorbar()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J0nALdxAhH4V","colab_type":"text"},"cell_type":"markdown","source":["## Logistic regression in TensorFlow\n","\n","https://gist.github.com/fuglede/ad04ce38e80887ddcbeb6b81e97bbfbc\n","\n"]},{"metadata":{"id":"07wEoayZhH4W","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}